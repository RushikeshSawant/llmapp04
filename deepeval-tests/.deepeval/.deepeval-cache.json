{"test_cases_lookup_map": {"{\"actual_output\": \"{\\\"labels\\\": [\\\"Python\\\", \\\"Programming\\\", \\\"New Features\\\"], \\\"primaryCategory\\\": \\\"Technology\\\", \\\"confidence\\\": 0.9}\", \"context\": null, \"expected_output\": \"{\\\"labels\\\": [\\\"technology\\\", \\\"programming\\\", \\\"software\\\"], \\\"primaryCategory\\\": \\\"technology\\\", \\\"confidence\\\": 0.9}\", \"hyperparameters\": null, \"input\": \"Python 3.12 introduces several new features including improved error messages, a new type parameter syntax, and performance improvements in the interpreter.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 0.8, "reason": "The output accurately identifies the main topic (Python programming with new features) and correctly categorizes it under 'Technology'. The confidence score of 0.9 indicates strong alignment with the input's content, reflecting the high relevance of the output to the provided test case.", "strictMode": false, "evaluationModel": "liquid/lfm2-1.2b", "evaluationCost": 0, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Assess if the output's main topic aligns with the input's subject matter.\",\n    \"Verify that all labels and categories in the output are directly applicable to the input's content.\",\n    \"Check if confidence scores reflect the relevance of the output to the input.\",\n    \"Compare structured metadata (labels, categories) in the output with those present in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.8"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "liquid/lfm2-1.2b", "strict_mode": false, "criteria": "Evaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant.", "include_reason": false, "evaluation_steps": ["Assess if the output's main topic aligns with the input's subject matter.", "Verify that all labels and categories in the output are directly applicable to the input's content.", "Check if confidence scores reflect the relevance of the output to the input.", "Compare structured metadata (labels, categories) in the output with those present in the input."], "evaluation_params": ["input", "actual_output"]}}]}, "{\"actual_output\": \"{\\\"labels\\\": [\\\"sports\\\", \\\"basketball\\\", \\\"game\\\"], \\\"primaryCategory\\\": \\\"sports\\\", \\\"confidence\\\": 0.9}\", \"context\": null, \"expected_output\": \"{\\\"labels\\\": [\\\"sports\\\", \\\"basketball\\\", \\\"NBA\\\"], \\\"primaryCategory\\\": \\\"sports\\\", \\\"confidence\\\": 0.9}\", \"hyperparameters\": null, \"input\": \"The Lakers defeated the Celtics 112-108 in overtime last night. LeBron James scored 35 points and had 10 assists in the victory.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": false, "score": 0.0, "reason": "The output focuses on a sports event (basketball game) but lacks the specificity of the input, which details a particular game outcome. While 'sports' and 'basketball' are present, the absence of key details like teams, score breakdown, or context reduces relevance. The confidence score of 0 indicates low alignment with the input's specific content.", "strictMode": false, "evaluationModel": "liquid/lfm2-1.2b", "evaluationCost": 0, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Assess if the output's main topic aligns with the input's subject matter.\",\n    \"Verify that all labels and categories in the output are directly applicable to the input's content.\",\n    \"Check if confidence scores reflect the relevance of the output to the input.\",\n    \"Compare structured metadata (labels, categories) in the output with those present in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.0"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "liquid/lfm2-1.2b", "strict_mode": false, "criteria": "Evaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant.", "include_reason": false, "evaluation_steps": ["Assess if the output's main topic aligns with the input's subject matter.", "Verify that all labels and categories in the output are directly applicable to the input's content.", "Check if confidence scores reflect the relevance of the output to the input.", "Compare structured metadata (labels, categories) in the output with those present in the input."], "evaluation_params": ["input", "actual_output"]}}]}, "{\"actual_output\": \"{\\\"labels\\\": [\\\"recipe\\\", \\\"cooking\\\"], \\\"primaryCategory\\\": \\\"food\\\", \\\"confidence\\\": 0.9}\", \"context\": null, \"expected_output\": \"{\\\"labels\\\": [\\\"food\\\", \\\"cooking\\\", \\\"recipe\\\"], \\\"primaryCategory\\\": \\\"food\\\", \\\"confidence\\\": 0.9}\", \"hyperparameters\": null, \"input\": \"To make a classic Italian carbonara, you need guanciale, eggs, pecorino romano cheese, black pepper, and spaghetti. Cook the pasta al dente and toss with the egg and cheese mixture.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": false, "score": 0.0, "reason": "The output focuses on cooking techniques for Italian carbonara, which is a dish, but lacks the specific ingredient and subject matter alignment. The primary category 'food' is broad, while the input is very specific to Italian cuisine. Confidence score of 0 indicates low relevance.", "strictMode": false, "evaluationModel": "liquid/lfm2-1.2b", "evaluationCost": 0, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Assess if the output's main topic aligns with the input's subject matter.\",\n    \"Verify that all labels and categories in the output are directly applicable to the input's content.\",\n    \"Check if confidence scores reflect the relevance of the output to the input.\",\n    \"Compare structured metadata (labels, categories) in the output with those present in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.0"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "liquid/lfm2-1.2b", "strict_mode": false, "criteria": "Evaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant.", "include_reason": false, "evaluation_steps": ["Assess if the output's main topic aligns with the input's subject matter.", "Verify that all labels and categories in the output are directly applicable to the input's content.", "Check if confidence scores reflect the relevance of the output to the input.", "Compare structured metadata (labels, categories) in the output with those present in the input."], "evaluation_params": ["input", "actual_output"]}}]}, "{\"actual_output\": \"{\\\"labels\\\": [\\\"economics\\\", \\\"finance\\\", \\\"Federal Reserve\\\"], \\\"primaryCategory\\\": \\\"economics\\\", \\\"confidence\\\": 0.9}\", \"context\": null, \"expected_output\": \"{\\\"labels\\\": [\\\"finance\\\", \\\"economics\\\", \\\"policy\\\"], \\\"primaryCategory\\\": \\\"finance\\\", \\\"confidence\\\": 0.9}\", \"hyperparameters\": null, \"input\": \"The Federal Reserve announced it will maintain interest rates at their current level, citing concerns about inflation and the labor market outlook for the coming quarter.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 0.8, "reason": "The output accurately identifies the main topic (economics) and correctly labels it as such. The primary category 'economics' is directly applicable to the input's subject matter. Confidence score reflects the high relevance of the output to the input, with 90% confidence indicating strong alignment.", "strictMode": false, "evaluationModel": "liquid/lfm2-1.2b", "evaluationCost": 0, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Assess if the output's main topic aligns with the input's subject matter.\",\n    \"Verify that all labels and categories in the output are directly applicable to the input's content.\",\n    \"Check if confidence scores reflect the relevance of the output to the input.\",\n    \"Compare structured metadata (labels, categories) in the output with those present in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.8"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "liquid/lfm2-1.2b", "strict_mode": false, "criteria": "Evaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant.", "include_reason": false, "evaluation_steps": ["Assess if the output's main topic aligns with the input's subject matter.", "Verify that all labels and categories in the output are directly applicable to the input's content.", "Check if confidence scores reflect the relevance of the output to the input.", "Compare structured metadata (labels, categories) in the output with those present in the input."], "evaluation_params": ["input", "actual_output"]}}]}, "{\"actual_output\": \"{\\\"labels\\\": [\\\"health\\\", \\\"exercise\\\", \\\"cardiovascular health\\\"], \\\"primaryCategory\\\": \\\"health\\\", \\\"confidence\\\": 0.9}\", \"context\": null, \"expected_output\": \"{\\\"labels\\\": [\\\"health\\\", \\\"science\\\", \\\"medical\\\"], \\\"primaryCategory\\\": \\\"health\\\", \\\"confidence\\\": 0.9}\", \"hyperparameters\": null, \"input\": \"A new study published in Nature shows that regular exercise can reduce the risk of heart disease by up to 30 percent and improve overall mental health outcomes.\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Answer Relevancy [GEval]", "threshold": 0.5, "success": true, "score": 0.8, "reason": "The output accurately identifies the main topic (reducing heart disease risk through exercise) and correctly categorizes it under 'health'. The confidence score of 0.9 indicates strong alignment with the input's content, particularly the emphasis on cardiovascular health and exercise's impact.", "strictMode": false, "evaluationModel": "liquid/lfm2-1.2b", "evaluationCost": 0, "verboseLogs": "Criteria:\nEvaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant. \n \nEvaluation Steps:\n[\n    \"Assess if the output's main topic aligns with the input's subject matter.\",\n    \"Verify that all labels and categories in the output are directly applicable to the input's content.\",\n    \"Check if confidence scores reflect the relevance of the output to the input.\",\n    \"Compare structured metadata (labels, categories) in the output with those present in the input.\"\n] \n \nRubric:\nNone \n \nScore: 0.8"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "liquid/lfm2-1.2b", "strict_mode": false, "criteria": "Evaluate whether the actual output is topically relevant to the input text. The labels, categories, or analysis in the output should directly relate to the subject matter of the input. Structured metadata (labels, categories, confidence scores) that accurately describes the input text should be considered relevant.", "include_reason": false, "evaluation_steps": ["Assess if the output's main topic aligns with the input's subject matter.", "Verify that all labels and categories in the output are directly applicable to the input's content.", "Check if confidence scores reflect the relevance of the output to the input.", "Compare structured metadata (labels, categories) in the output with those present in the input."], "evaluation_params": ["input", "actual_output"]}}]}}}